{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FreyHW2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"toc":{"base_numbering":1,"nav_menu":{},"number_sections":true,"sideBar":true,"skip_h1_title":false,"title_cell":"Table of Contents","title_sidebar":"Contents","toc_cell":true,"toc_position":{},"toc_section_display":true,"toc_window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"0QJDNuifaIwP","toc":true},"source":["<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n","<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Overview\" data-toc-modified-id=\"Overview-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Overview</a></span></li><li><span><a href=\"#Notebook-Configuration\" data-toc-modified-id=\"Notebook-Configuration-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Notebook Configuration</a></span><ul class=\"toc-item\"><li><span><a href=\"#Google-drive\" data-toc-modified-id=\"Google-drive-2.1\"><span class=\"toc-item-num\">2.1&nbsp;&nbsp;</span>Google drive</a></span></li><li><span><a href=\"#Warning\" data-toc-modified-id=\"Warning-2.2\"><span class=\"toc-item-num\">2.2&nbsp;&nbsp;</span>Warning</a></span></li><li><span><a href=\"#Matplotlib\" data-toc-modified-id=\"Matplotlib-2.3\"><span class=\"toc-item-num\">2.3&nbsp;&nbsp;</span>Matplotlib</a></span></li><li><span><a href=\"#TensorFlow\" data-toc-modified-id=\"TensorFlow-2.4\"><span class=\"toc-item-num\">2.4&nbsp;&nbsp;</span>TensorFlow</a></span></li><li><span><a href=\"#Random-seed\" data-toc-modified-id=\"Random-seed-2.5\"><span class=\"toc-item-num\">2.5&nbsp;&nbsp;</span>Random seed</a></span></li></ul></li><li><span><a href=\"#Data-Preprocessing\" data-toc-modified-id=\"Data-Preprocessing-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Data Preprocessing</a></span></li><li><span><a href=\"#Hyperparameter-Tuning\" data-toc-modified-id=\"Hyperparameter-Tuning-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Hyperparameter Tuning</a></span></li><li><span><a href=\"#Model-Selection\" data-toc-modified-id=\"Model-Selection-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Model Selection</a></span></li><li><span><a href=\"#Generating-the-Submission-File\" data-toc-modified-id=\"Generating-the-Submission-File-6\"><span class=\"toc-item-num\">6&nbsp;&nbsp;</span>Generating the Submission File</a></span><ul class=\"toc-item\"><li><span><a href=\"#Creating-the-directory-for-the-submission-file\" data-toc-modified-id=\"Creating-the-directory-for-the-submission-file-6.1\"><span class=\"toc-item-num\">6.1&nbsp;&nbsp;</span>Creating the directory for the submission file</a></span></li><li><span><a href=\"#Generating-the-submission-file\" data-toc-modified-id=\"Generating-the-submission-file-6.2\"><span class=\"toc-item-num\">6.2&nbsp;&nbsp;</span>Generating the submission file</a></span></li></ul></li></ul></div>"]},{"cell_type":"markdown","metadata":{"id":"42yc1zUrzlEh"},"source":["<b>\n","<p>\n","<center>\n","<font size=\"5\">\n","Popular Machine Learning Methods: Idea, Practice and Math\n","</font>\n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"4\">\n","Part 2, Chapter 2, Section 4: Shallow Neural Networks\n","</font>\n","</center>\n","</p>\n","    \n","<p>\n","<center>\n","<font size=\"4\">\n","Homework 2\n","</font>\n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"3\">\n","Data Science, Columbian College of Arts & Sciences, George Washington University\n","</font>\n","</center>\n","</p>\n","\n","<p>\n","<center>\n","<font size=\"3\">\n","Yuxiao Huang\n","</font>\n","</center>\n","</p>\n","</b>"]},{"cell_type":"markdown","metadata":{"id":"rT2SKHw2zlEi"},"source":["# Overview"]},{"cell_type":"markdown","metadata":{"id":"asZWLrJKzlEj"},"source":["- This notebook includes homework 2 for Shallow Neural Networks (Part 2, Chapter 2, Section 4).\n","- See the accompanied slides in our [github repository](https://github.com/yuxiaohuang/teaching/tree/master/gwu/machine_learning_I/fall_2020/slides/p2_shallow_learning/p2_c2_supervised_learning/p2_c2_s4_shallow_neural_networks).\n","- Here we will work on kaggle competation [Santander Customer Transaction Prediction](https://www.kaggle.com/c/santander-customer-transaction-prediction/overview)\n","- The goal of this homework is tweaking the pipeline (including data preprocessing, hyperparameter tuning and model selection) implemented in [/p2 c2 s4 shallow neural_networks/case_study](https://github.com/yuxiaohuang/teaching/blob/master/gwu/machine_learning_I/fall_2020/code/p2_shallow_learning/p2_c2_supervised_learning/p2_c2_s4_shallow_neural_networks/case_study/case_study.ipynb) to make it work for the new kaggle competation mentioned above.\n","- Complete the missing parts indicated by # Implement me.\n","- Particularly, the code should\n","    - be bug-free (note that the output produced by your solution may not necessarily be the same as the provided output, due to version issues)\n","    - be commented\n","- **Marks will be deducted if the above requirements (for the code) are not met**.\n","- Submit an ipynb file named homework_2.ipynb to [blackboard](https://blackboard.gwu.edu) folder /Assignments/Homework_2/."]},{"cell_type":"markdown","metadata":{"id":"WJ-IbZqAgILJ"},"source":["# Notebook Configuration"]},{"cell_type":"markdown","metadata":{"id":"H3yB94KtgMHu"},"source":["## Google drive"]},{"cell_type":"code","metadata":{"id":"jWmYBTOwgNs-","executionInfo":{"status":"ok","timestamp":1603165164797,"user_tz":240,"elapsed":985,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}},"outputId":"cb001b96-6017-4dda-9491-a69e44006eb0","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["### Matt Frey\n","### Dr. Huang\n","### GWU Machine Learning\n","\n","from google.colab import drive\n","import sys\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Get the absolute path of the current folder\n","abspath_curr = '/content/drive/My Drive/Colab Notebooks/ML/HW2'\n","\n","# Get the absolute path of the shallow utilities folder\n","abspath_util_shallow = '/content/drive/My Drive/Colab Notebooks/ML/HW1/code/utilities/p2_shallow_learning/'\n","\n","# Get the absolute path of the shallow models folder\n","abspath_model_shallow = '/content/drive/My Drive/Colab Notebooks/ML/HW1/code/models/p2_shallow_learning/'"],"execution_count":96,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bYZhU1Wqgmqx"},"source":["## Warning"]},{"cell_type":"code","metadata":{"id":"MUl4k83e4ANR","executionInfo":{"status":"ok","timestamp":1603165164798,"user_tz":240,"elapsed":975,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}}},"source":["import warnings\n","\n","# Ignore warnings\n","warnings.filterwarnings('ignore')"],"execution_count":97,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6WMODpPfgn2U"},"source":["## Matplotlib"]},{"cell_type":"code","metadata":{"id":"DBRVH9SB4ANb","executionInfo":{"status":"ok","timestamp":1603165164799,"user_tz":240,"elapsed":967,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline \n","\n","# Set matplotlib sizes\n","plt.rc('font', size=20)\n","plt.rc('axes', titlesize=20)\n","plt.rc('axes', labelsize=20)\n","plt.rc('xtick', labelsize=20)\n","plt.rc('ytick', labelsize=20)\n","plt.rc('legend', fontsize=20)\n","plt.rc('figure', titlesize=20)"],"execution_count":98,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-wNDk5nZhhO"},"source":["## TensorFlow"]},{"cell_type":"code","metadata":{"id":"LjG43tEnZkfE","executionInfo":{"status":"ok","timestamp":1603165164800,"user_tz":240,"elapsed":957,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}}},"source":["# The magic below allows us to use tensorflow version 2.x\n","%tensorflow_version 2.x \n","import tensorflow as tf\n","from tensorflow import keras"],"execution_count":99,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40FN3UNfO2Z7"},"source":["## Random seed"]},{"cell_type":"code","metadata":{"id":"uSADk0hJP71d","executionInfo":{"status":"ok","timestamp":1603165164801,"user_tz":240,"elapsed":952,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}}},"source":["# The random seed\n","random_seed = 42\n","\n","# Set random seed in tensorflow\n","tf.random.set_seed(random_seed)\n","\n","# Set random seed in numpy\n","import numpy as np\n","np.random.seed(random_seed)"],"execution_count":100,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yAwfz8iYzlFC"},"source":["# Data Preprocessing"]},{"cell_type":"code","metadata":{"id":"D-Mwc6MczlFD","executionInfo":{"status":"ok","timestamp":1603165165068,"user_tz":240,"elapsed":1209,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}},"outputId":"ec6278c3-3d56-4a06-f602-5c8ea6def954","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Change working directory to the absolute path of the shallow utilities folder\n","%cd $abspath_util_shallow\n","\n","# Import the shallow utitilities\n","%run pmlm_utilities_shallow.ipynb"],"execution_count":101,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/Colab Notebooks/ML/HW1/code/utilities/p2_shallow_learning\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GMm53nLtlx_G","executionInfo":{"status":"ok","timestamp":1603165183162,"user_tz":240,"elapsed":19296,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}}},"source":["### Loading the data ###\n","\n","import pandas as pd\n","\n","# Load the raw training data\n","df_raw_train = pd.read_csv(abspath_curr + '/data/train.csv',\n","                           header=0)\n","# Make a copy of df_raw_train\n","df_train = df_raw_train.copy(deep=True)\n","\n","# Load the raw test data\n","df_raw_test = pd.read_csv(abspath_curr + '/data/test.csv',\n","                          header=0)\n","# Make a copy of df_raw_test\n","df_test = df_raw_test.copy(deep=True)\n","\n","# Get the name of the target\n","target = 'var_0'\n","\n","# Print the dimension of df_train\n","pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_test\n","pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the first 5 rows of df_train\n","df_train.head()\n","\n","# Print the first 5 rows of df_test\n","df_test.head()\n","\n","### Splitting the data ###\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Divide the training data into training (80%) and validation (20%)\n","df_train, df_val = train_test_split(df_train, train_size=0.8, random_state=random_seed)\n","\n","# Reset the index\n","df_train, df_val = df_train.reset_index(drop=True), df_val.reset_index(drop=True)\n","\n","# Print the dimension of df_train\n","pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_val\n","pd.DataFrame([[df_val.shape[0], df_val.shape[1]]], columns=['# rows', '# columns'])\n","\n","### Handling uncommon features ###\n","\n","# Call common_var_checker\n","# See the implementation in pmlm_utilities.ipynb\n","df_common_var = common_var_checker(df_train, df_val, df_test, target)\n","\n","# Print df_common_var\n","df_common_var\n","\n","# Get the features in the training data but not in the validation or test data\n","uncommon_feature_train_not_val_test = np.setdiff1d(df_train.columns, df_common_var['common var'])\n","\n","# Print the uncommon features\n","pd.DataFrame(uncommon_feature_train_not_val_test, columns=['uncommon feature'])\n","\n","# Get the features in the validation data but not in the training or test data\n","uncommon_feature_val_not_train_test = np.setdiff1d(df_val.columns, df_common_var['common var'])\n","\n","# Print the uncommon features\n","pd.DataFrame(uncommon_feature_val_not_train_test, columns=['uncommon feature'])\n","\n","# Get the features in the test data but not in the training or validation data\n","uncommon_feature_test_not_train_val = np.setdiff1d(df_test.columns, df_common_var['common var'])\n","\n","# Print the uncommon features\n","pd.DataFrame(uncommon_feature_test_not_train_val, columns=['uncommon feature'])\n","\n","### Removing uncommon features ###\n","\n","# Remove the uncommon features from the training data\n","df_train = df_train.drop(columns=uncommon_feature_train_not_val_test)\n","\n","# Print the first 5 rows of df_train\n","df_train.head()\n","\n","# Remove the uncommon features from the validation data\n","df_val = df_val.drop(columns=uncommon_feature_val_not_train_test)\n","\n","# Print the first 5 rows of df_val\n","df_val.head()\n","\n","# Remove the uncommon features from the test data\n","df_test = df_test.drop(columns=uncommon_feature_test_not_train_val)\n","\n","# Print the first 5 rows of df_test\n","df_test.head()\n","\n","### Handling identifiers ###\n","\n","# Combine df_train, df_val and df_test\n","df = pd.concat([df_train, df_val, df_test], sort=False)\n","\n","# Call id_checker on df\n","# See the implementation in pmlm_utilities.ipynb\n","df_id = id_checker(df)\n","\n","# Print the first 5 rows of df_id\n","df_id.head()\n","\n","### Removing identifiers\n","\n","import numpy as np\n","\n","# Remove identifiers from df_train\n","df_train.drop(columns=np.intersect1d(df_id.columns, df_train.columns), inplace=True)\n","\n","# Remove identifiers from df_val\n","df_val.drop(columns=np.intersect1d(df_id.columns, df_val.columns), inplace=True)\n","\n","# Remove identifiers from df_test\n","df_test.drop(columns=np.intersect1d(df_id.columns, df_test.columns), inplace=True)\n","\n","# Print the first 5 rows of df_train\n","df_train.head()\n","\n","# Print the first 5 rows of df_val\n","df_val.head()\n","\n","# Print the first 5 rows of df_test\n","df_test.head()\n","\n","### Handling missing data ###\n","\n","# Combine df_train, df_val and df_test\n","df = pd.concat([df_train, df_val, df_test], sort=False)\n","\n","# Call nan_checker on df\n","# See the implementation in pmlm_utilities.ipynb\n","df_nan = nan_checker(df)\n","\n","# Print df_nan\n","df_nan\n","\n","# Print the unique data type of variables with NaN\n","pd.DataFrame(df_nan['dtype'].unique(), columns=['dtype'])\n","\n","# Get the variables with missing values, their proportion of missing values and data type\n","df_miss = df_nan[df_nan['dtype'] == 'float64'].reset_index(drop=True)\n","\n","# Print df_miss\n","df_miss\n","\n","# Separating the training data\n","df_train = df.iloc[:df_train.shape[0], :]\n","\n","# Separating the validation data\n","df_val = df.iloc[df_train.shape[0]:df_train.shape[0] + df_val.shape[0], :]\n","\n","# Separating the test data\n","df_test = df.iloc[df_train.shape[0] + df_val.shape[0]:, :]\n","\n","# Print the dimension of df_train\n","pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_val\n","pd.DataFrame([[df_val.shape[0], df_val.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_test\n","pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])\n","\n","### Encoding the data ###\n","\n","# Combine df_train, df_val and df_test\n","df = pd.concat([df_train, df_val, df_test], sort=False)\n","\n","# Print the unique data type of variables in df\n","pd.DataFrame(df.dtypes.unique(), columns=['dtype'])\n","\n","### Separating the training, validation and test data ###\n","\n","# Separating the training data\n","df_train = df.iloc[:df_train.shape[0], :]\n","\n","# Separating the validation data\n","df_val = df.iloc[df_train.shape[0]:df_train.shape[0] + df_val.shape[0], :]\n","\n","# Separating the test data\n","df_test = df.iloc[df_train.shape[0] + df_val.shape[0]:, :]\n","\n","# Print the dimension of df_train\n","pd.DataFrame([[df_train.shape[0], df_train.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_val\n","pd.DataFrame([[df_val.shape[0], df_val.shape[1]]], columns=['# rows', '# columns'])\n","\n","# Print the dimension of df_test\n","pd.DataFrame([[df_test.shape[0], df_test.shape[1]]], columns=['# rows', '# columns'])\n","\n","### Splitting the feature and target ###\n","\n","# Get the feature matrix\n","X_train = df_train[np.setdiff1d(df_train.columns, [target])].values\n","X_val = df_val[np.setdiff1d(df_val.columns, [target])].values\n","X_test = df_test[np.setdiff1d(df_test.columns, [target])].values\n","\n","# Get the target vector\n","y_train = df_train[target].values\n","y_val = df_val[target].values\n","y_test = df_test[target].values\n","\n","### Scaling the data ### \n","\n","from sklearn.preprocessing import MinMaxScaler\n","\n","# The MinMaxScaler\n","mms = MinMaxScaler()\n","\n","# Normalize the training data\n","X_train = mms.fit_transform(X_train)\n","\n","# Normalize the validation data\n","X_val = mms.transform(X_val)\n","\n","# Normalize the test data\n","X_test = mms.transform(X_test)"],"execution_count":102,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Of56ObPGe31x"},"source":["# Hyperparameter Tuning"]},{"cell_type":"code","metadata":{"id":"2ICnjl3Olx_I","executionInfo":{"status":"error","timestamp":1603165186955,"user_tz":240,"elapsed":23080,"user":{"displayName":"Matthew Frey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3bZGhSxvADIcHt9fDf31JaXXH3fKvx63t_TlImKo=s64","userId":"17189771300403621797"}},"outputId":"6ee1e121-d0b2-43af-dbee-b1355b70eed4","colab":{"base_uri":"https://localhost:8080/","height":375}},"source":["### Creating the dictionary of the models ###\n","\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neural_network import MLPClassifier\n","\n","models = {'lr': LogisticRegression(class_weight='balanced', random_state=random_seed),\n","          'mlpc': MLPClassifier(early_stopping=True, random_state=random_seed)}\n","\n","### Creating the dictionary of the pipelines ###\n","\n","from sklearn.pipeline import Pipeline\n","\n","pipes = {}\n","\n","for acronym, model in models.items():\n","    pipes[acronym] = Pipeline([('model', model)])\n","\n","### Getting the predefined split cross-validator ### \n","\n","# Get the:\n","# feature matrix and target velctor in the combined training and validation data\n","# target vector in the combined training and validation data\n","# PredefinedSplit\n","# See the implementation in pmlm_utilities.ipynb\n","X_train_val, y_train_val, ps = get_train_val_ps(X_train, y_train, X_val, y_val)\n","\n","### GridSearchCV ###\n","\n","param_grids = {}\n","\n","# The parameter grid of tol\n","tol_grid = [10 ** -5, 10 ** -4, 10 ** -3]\n","\n","# The parameter grid of C\n","C_grid = [0.1, 1, 10]\n","\n","# Update param_grids\n","param_grids['lr'] = [{'model__tol': tol_grid,\n","                      'model__C': C_grid}]\n","\n","### Creating the directory for the cv results produced by GridSearchCV ###\n","\n","# Make directory\n","directory = os.path.dirname(abspath_curr)\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","### Tuning ###\n","\n","from sklearn.model_selection import GridSearchCV\n","\n","# The list of [best_score_, best_params_, best_estimator_] obtained by GridSearchCV\n","best_score_params_estimator_gs = []\n","\n","# For each model\n","for acronym in pipes.keys():\n","    # GridSearchCV\n","    gs = GridSearchCV(estimator=pipes[acronym],\n","                      param_grid=param_grids[acronym],\n","                      scoring='f1_macro',\n","                      n_jobs=2,\n","                      cv=ps,\n","                      return_train_score=True)\n","        \n","    # Fit the pipeline\n","    gs = gs.fit(X_train_val, y_train_val)\n","    \n","    # Update best_score_params_estimator_gs\n","    best_score_params_estimator_gs.append([gs.best_score_, gs.best_params_, gs.best_estimator_])\n","    \n","    # Sort cv_results in ascending order of 'rank_test_score' and 'std_test_score'\n","    cv_results = pd.DataFrame.from_dict(gs.cv_results_).sort_values(by=['rank_test_score', 'std_test_score'])\n","    \n","    # Get the important columns in cv_results\n","    important_columns = ['rank_test_score',\n","                         'mean_test_score', \n","                         'std_test_score', \n","                         'mean_train_score', \n","                         'std_train_score',\n","                         'mean_fit_time', \n","                         'std_fit_time',                        \n","                         'mean_score_time', \n","                         'std_score_time']\n","    \n","    # Move the important columns ahead\n","    cv_results = cv_results[important_columns + sorted(list(set(cv_results.columns) - set(important_columns)))]\n","\n","    # Write cv_results file\n","    cv_results.to_csv(path_or_buf=abspath_curr + acronym + '.csv', index=False)\n","\n","# Sort best_score_params_estimator_gs in descending order of the best_score_\n","best_score_params_estimator_gs = sorted(best_score_params_estimator_gs, key=lambda x : x[0], reverse=True)\n","\n","# Print best_score_params_estimator_gs\n","pd.DataFrame(best_score_params_estimator_gs, columns=['best_score', 'best_param', 'best_estimator'])"],"execution_count":103,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-103-d549b3a6b0c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;31m# Fit the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Update best_score_params_estimator_gs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0mrefit_start_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 739\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    740\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    352\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 354\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    355\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[1;32m   1527\u001b[0m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0;32m-> 1528\u001b[0;31m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1530\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/multiclass.py\u001b[0m in \u001b[0;36mcheck_classification_targets\u001b[0;34m(y)\u001b[0m\n\u001b[1;32m    167\u001b[0m     if y_type not in ['binary', 'multiclass', 'multiclass-multioutput',\n\u001b[1;32m    168\u001b[0m                       'multilabel-indicator', 'multilabel-sequences']:\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Unknown label type: %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Unknown label type: 'continuous'"]}]},{"cell_type":"markdown","metadata":{"id":"kOF6id8le32I"},"source":["# Model Selection"]},{"cell_type":"code","metadata":{"id":"c2O8-Fv9lx_J"},"source":["# Get the best_score, best_params and best_estimator obtained by GridSearchCV\n","best_score_gs, best_params_gs, best_estimator_gs = best_score_params_estimator_gs[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fiEBpqc_e32K"},"source":["# Generating the Submission File\n","Use the best model selected earlier to generate the submission file for this kaggle competition."]},{"cell_type":"code","metadata":{"id":"LbFrfK3NAPVk"},"source":["# Make directory\n","directory = os.path.dirname(abspath_curr)\n","if not os.path.exists(directory):\n","    os.makedirs(directory)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y-Qf_8VM0Tt9"},"source":["## Generating the submission file"]},{"cell_type":"code","metadata":{"id":"mVdD3N0re32L","scrolled":true},"source":["# Get the prediction on the test data using the best model\n","y_test_pred = best_estimator_gs.predict(X_test)\n","\n","# Transform y_test_pred back to the original class\n","y_test_pred = le.inverse_transform(y_test_pred)\n","\n","# Get the submission dataframe\n","df_submit = pd.DataFrame(np.hstack((df_raw_test[['ID_code']], y_test_pred.reshape(-1, 1))),\n","                         columns=['ID_code', target])                                                                                     \n","\n","# Generate the submission file\n","df_submit.to_csv(abspath_curr, index=False)"],"execution_count":null,"outputs":[]}]}